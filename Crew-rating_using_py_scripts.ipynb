{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled crew data from Pixel Starships API.\n",
      "Wrote data to excel file.\n",
      "Getting sprite images...\n",
      "sprites/387.png\n",
      "sprites/9064.png\n",
      "sprites/9754.png\n",
      "Wrote data to Wordpress db.\n",
      "Here's your data frame!\n"
     ]
    }
   ],
   "source": [
    "from py_files import get_crew_from_api\n",
    "crew_df = get_crew_from_api.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GET PRESTIGE DATA\n",
    "# from py_files import get_prestige_from_api\n",
    "# prestige_df = get_prestige_from_api.main(crew_df['CharacterDesignId'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET SPRITE IMAGES\n",
    "import urllib\n",
    "import os.path\n",
    "baseSprite_url = 'http://apibackup.pixelstarships.com/FileService/DownloadSprite?spriteId='\n",
    "\n",
    "for index, row in crew_df.iterrows():\n",
    "    # head\n",
    "    url = baseSprite_url + str(row['Head'])\n",
    "    filename = 'sprites/' + str(row['Head']) + '.png'\n",
    "    if os.path.isfile(filename) == False:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "    # body\n",
    "    url = baseSprite_url + str(row['Body'])\n",
    "    filename = 'sprites/' + str(row['Body']) + '.png'\n",
    "    if os.path.isfile(filename) == False:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(filename)\n",
    "    \n",
    "    # leg\n",
    "    url = baseSprite_url + str(row['Leg'])\n",
    "    filename = 'sprites/' + str(row['Leg']) + '.png'\n",
    "    if os.path.isfile(filename) == False:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_files import get_manual_grades\n",
    "grades_df = get_manual_grades.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_files import prep_model_features\n",
    "feats_df = prep_model_features.main(crew_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_files import create_model_sets\n",
    "train_features, train_labels, test_features, test_labels = create_model_sets.main(feats_df, grades_df, 'Engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(model, role, feats_df, grades_df):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    # import seaborn as sns\n",
    "    # sns.set()\n",
    "    \n",
    "    train_features, train_labels, test_features, test_labels = create_model_sets.main(feats_df, grades_df, role, p=False)\n",
    "    \n",
    "    # run the model on the train/test sets    \n",
    "    model.fit(train_features, train_labels)\n",
    "    print(\"\\n The model achieves an R2 value of \" + str(model.score(test_features, test_labels)) + \" on the \" + role + \" test set.\")\n",
    "    y_pred_test = model.predict(test_features)\n",
    "    \n",
    "    # for each character in the grades list, put the label in the \"grade\" column\n",
    "    df = feats_df.copy()\n",
    "    df['grade'] = None  # default is no grade\n",
    "       \n",
    "    for ind in range(len(grades_df)):\n",
    "        g = grades_df[f\"{role + 'Input'}\"].values[ind]\n",
    "        df.set_value(df.CharacterDesignId == grades_df['CharacterDesignId'].values[ind], 'grade', g)\n",
    "    \n",
    "    graded_crew = df[df.grade.isnull() == False].reset_index(drop=True)\n",
    "    y_pred_all = model.predict(graded_crew.drop(['CharacterDesignId', 'grade'], axis=1))\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize = (40,10))\n",
    "    boundaries = [1.5, 2.5, 3.5]\n",
    "    \n",
    "    # plot test grades\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title('Test Set Grading')\n",
    "    plt.xlabel('manual grades')\n",
    "    plt.ylabel('model grades')\n",
    "    plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred_test))*0.2, y_pred_test, marker ='+', color = 'blue')\n",
    "    for v in boundaries:\n",
    "        plt.plot([0,5], [v,v], color='gray')\n",
    "    \n",
    "    # plot all grades\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title('All Crew Grading')\n",
    "    plt.xlabel('manual grades')\n",
    "    plt.ylabel('model grades')\n",
    "    plt.scatter(graded_crew['grade']+0.1-np.random.uniform(size=len(y_pred_all))*0.2, y_pred_all, marker ='+', color = 'blue')\n",
    "    for v in boundaries:\n",
    "        plt.plot([0,5], [v,v], color='gray')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    return graded_crew['CharacterDesignId'], graded_crew['grade'], y_pred_all;\n",
    "\n",
    "\n",
    "    # print labels that would be written wrong (wouldn't round to the right grade)\n",
    "    for ind in range(len(y_pred_all)):\n",
    "        if graded_crew['grade'].values[ind] > 1.9:  # only worry about 2,3,4 grades. 0 and 1 aren't going into the post\n",
    "            if round(y_pred_all[ind], 0) != graded_crew['grade'].values[ind]:\n",
    "                print('Prediction error: CrewId' + str(df['CharacterDesignId'].values[ind]) \n",
    "                      + ' should be ' + str(df['grade'].values[ind]) + ' but the model predicts ' + str(round(y_pred_all[ind], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_errors(ids, manual_grades, model_grades, crew_df):\n",
    "        for ind in range(len(ids)):\n",
    "            if manual_grades[ind] > 1.9:  # only worry about 2,3,4 grades. 0 and 1 aren't going into the post\n",
    "                if round(model_grades[ind], 0) != manual_grades[ind]:\n",
    "                    # bad grading\n",
    "                    crewloc = crew_df['CharacterDesignId'].values.tolist().index(ids[ind])\n",
    "                    crewname = crew_df['CharacterDesignName'].values[crewloc]\n",
    "                    print('Prediction error: ' + crewname + \n",
    "                          ' should be ' + str(manual_grades[ind]) + ' but the model predicts ' + \n",
    "                          str(round(model_grades[ind], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# linear regression model\n",
    "Regmodel = LinearRegression()\n",
    "ids, manual_g, model_g = eval_model(Regmodel, 'Gunner', feats_df, grades_df)\n",
    "show_errors(ids, manual_g, model_g, crew_df)\n",
    "\n",
    "ids, manual_g, model_g = eval_model(Regmodel, 'Shielder', feats_df, grades_df)\n",
    "show_errors(ids, manual_g, model_g, crew_df)\n",
    "\n",
    "ids, manual_g, model_g = eval_model(Regmodel, 'Engineer', feats_df, grades_df)\n",
    "show_errors(ids, manual_g, model_g, crew_df)\n",
    "\n",
    "ids, manual_g, model_g = eval_model(Regmodel, 'Pilot', feats_df, grades_df)\n",
    "show_errors(ids, manual_g, model_g, crew_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what are the most important features?\n",
    "coefs = Regmodel.coef_\n",
    "top_feat_inds = np.fliplr([np.argsort(coefs)[-7:]])[0]\n",
    "bottom_feat_inds = np.argsort(coefs)[0:7]\n",
    "\n",
    "# features that bring rating up\n",
    "for i in top_feat_inds:\n",
    "    print(\"The feature \" + feats_df.columns.tolist()[i+1] +   # one label later, since we dropped CrewId after model_data\n",
    "          \" has a coefficient of \" +  str(coefs[i]))\n",
    "print(\"\\n\")\n",
    "# features that bring rating down\n",
    "for i in bottom_feat_inds:\n",
    "    print(\"The feature \" + feats_df.columns.tolist()[i+1] + \n",
    "          \" has a coefficient of \" +  str(coefs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kNN = KNeighborsClassifier(n_neighbors = 1)\n",
    "ids, manual_g, model_g = eval_model(kNN, 'Gunner', feats_df, grades_df)\n",
    "show_errors(ids, manual_g, model_g, crew_df)\n",
    "\n",
    "ids, manual_g, model_g = eval_model(kNN, 'Shielder', feats_df, grades_df)\n",
    "show_errors(ids, manual_g, model_g, crew_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_values = [1,3,5,7,9]\n",
    "\n",
    "for k in k_values:  \n",
    "    \n",
    "    kNN = KNeighborsClassifier(n_neighbors = k)\n",
    "    kNN.fit(train_features, train_labels)\n",
    "    \n",
    "    # take a stab at the test set\n",
    "    y_pred = kNN.predict(test_features)\n",
    "    # accuracy\n",
    "    acc = round(sum(y_pred == test_labels)/test_labels.shape[0],3)*100\n",
    "    \n",
    "    # what if you consider +/- 1 star to be still \"accurate\"?\n",
    "    acc2 = round((\n",
    "        sum(y_pred == test_labels) + \n",
    "        sum(y_pred == test_labels-1) + sum(y_pred == test_labels+1)\n",
    "        )/test_labels.shape[0],3)*100\n",
    "    print(\"The kNN classifier with k=\" + str(k) + \" had an accuracy of \" \n",
    "          + str(acc) + \"% (and a +/-1 grade accuracy of \" + str(acc2) + \"%).\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the best model: n = 1\n",
    "kNN = KNeighborsClassifier(n_neighbors = 1)\n",
    "kNN.fit(train_features, train_labels)\n",
    "\n",
    "# print the values the model gets wrong\n",
    "print(\"\\nThe ratings misclassified by the classifier were: \")\n",
    "y_pred = kNN.predict(test_features)\n",
    "errors = test_labels[y_pred != test_labels]\n",
    "print(str(y_pred[y_pred != test_labels]) + \"-- predicted values\")\n",
    "print(str(errors) + \"-- actual values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred))*0.2, y_pred+0.1-np.random.uniform(size=len(y_pred))*0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# logistic regression model\n",
    "LogRegmodel = LogisticRegression(multi_class = 'multinomial', solver='newton-cg')\n",
    "LogRegmodel.fit(train_features, train_labels)\n",
    "\n",
    "print(\"The logistic regression model achieves an R2 value of \" + str(LogRegmodel.score(test_features, test_labels)))\n",
    "\n",
    "# print the values the model gets wrong\n",
    "print(\"\\nThe ratings misclassified by the classifier were: \")\n",
    "y_pred = LogRegmodel.predict(test_features)\n",
    "errors = test_labels[y_pred != test_labels]\n",
    "print(str(y_pred[y_pred != test_labels]) + \"-- predicted values\")\n",
    "print(str(errors) + \"-- actual values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred))*0.2, y_pred+0.1-np.random.uniform(size=len(y_pred))*0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(train_features, train_labels)\n",
    "\n",
    "print(\"The perceptron model achieves an R2 value of \" + str(perceptron.score(test_features, test_labels)))\n",
    "y_pred = perceptron.predict(test_features)\n",
    "\n",
    "# print the values the model gets wrong\n",
    "print(\"\\nThe ratings misclassified by the classifier were: \")\n",
    "y_pred = perceptron.predict(test_features)\n",
    "errors = test_labels[y_pred != test_labels]\n",
    "print(str(y_pred[y_pred != test_labels]) + \"-- predicted values\")\n",
    "print(str(errors) + \"-- actual values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='poly', degree=8)\n",
    "svc.fit(train_features, train_labels)\n",
    "\n",
    "print(\"The svm model achieves an R2 value of \" + str(svc.score(test_features, test_labels)))\n",
    "y_pred = svc.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred))*0.2, y_pred+0.1-np.random.uniform(size=len(y_pred))*0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(train_features, train_labels)\n",
    "\n",
    "print(\"The decision tree achieves an R2 value of \" + str(tree.score(test_features, test_labels)))\n",
    "y_pred = tree.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred))*0.2, y_pred+0.1-np.random.uniform(size=len(y_pred))*0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
