{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled crew data from Pixel Starships API.\n",
      "Wrote data to excel file.\n",
      "Wrote data to Wordpress db.\n",
      "Here's your data frame!\n"
     ]
    }
   ],
   "source": [
    "from py_files import get_crew_from_api\n",
    "crew_df = get_crew_from_api.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading prestige data...\n",
      "Setting up data frame...\n",
      "Saving to wordpress database...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from py_files import get_prestige_from_api\n",
    "prestige_df = get_prestige_from_api.main(crew_df['CharacterDesignId'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6289 entries, 0 to 52\n",
      "Data columns (total 3 columns):\n",
      "CharacterDesignId1     6289 non-null int32\n",
      "CharacterDesignId2     6289 non-null int32\n",
      "ToCharacterDesignId    6289 non-null int32\n",
      "dtypes: int32(3)\n",
      "memory usage: 122.8 KB\n"
     ]
    }
   ],
   "source": [
    "prestige_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved manual grades from wordpress\n"
     ]
    }
   ],
   "source": [
    "from py_files import get_manual_grades\n",
    "grades_df = get_manual_grades.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing crews' ability scores...\n",
      "Creating binary category variables...\n",
      "Scaling numeric variables...\n",
      "Ready for modeling!\n"
     ]
    }
   ],
   "source": [
    "from py_files import prep_model_features\n",
    "feats_df = prep_model_features.main(crew_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching up grades with crew...\n",
      "Removing ungraded crew...\n",
      "The size of the model-able data is:\n",
      "(276, 40)\n",
      "Randomly selecting data sets...\n",
      "We will use 220 training points and 55 test points.\n"
     ]
    }
   ],
   "source": [
    "from py_files import create_model_sets\n",
    "train_features, train_labels, test_features, test_labels = create_model_sets.main(feats_df, grades_df, 'Shielder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# linear regression model\n",
    "Regmodel = LinearRegression()\n",
    "Regmodel.fit(train_features, train_labels)\n",
    "\n",
    "print(\"The linear regression model achieves an R2 value of \" + str(Regmodel.score(test_features, test_labels)))\n",
    "\n",
    "y_pred = Regmodel.predict(test_features)\n",
    "print(\"At worst, the predicted score was off by \" + str(round(np.max(y_pred - test_labels),3)) + \" grades.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred))*0.2, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the most important features?\n",
    "coefs = Regmodel.coef_\n",
    "top_feat_inds = np.fliplr([np.argsort(coefs)[-7:]])[0]\n",
    "bottom_feat_inds = np.argsort(coefs)[0:7]\n",
    "\n",
    "# features that bring rating up\n",
    "for i in top_feat_inds:\n",
    "    print(\"The feature \" + feats_df.columns.tolist()[i+1] +   # one label later, since we dropped CrewId after model_data\n",
    "          \" has a coefficient of \" +  str(coefs[i]))\n",
    "print(\"\\n\")\n",
    "# features that bring rating down\n",
    "for i in bottom_feat_inds:\n",
    "    print(\"The feature \" + feats_df.columns.tolist()[i+1] + \n",
    "          \" has a coefficient of \" +  str(coefs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_values = [1,3,5,7,9]\n",
    "\n",
    "for k in k_values:  \n",
    "    \n",
    "    kNN = KNeighborsClassifier(n_neighbors = k)\n",
    "    kNN.fit(train_features, train_labels)\n",
    "    \n",
    "    # take a stab at the test set\n",
    "    y_pred = kNN.predict(test_features)\n",
    "    # accuracy\n",
    "    acc = round(sum(y_pred == test_labels)/test_labels.shape[0],3)*100\n",
    "    \n",
    "    # what if you consider +/- 1 star to be still \"accurate\"?\n",
    "    acc2 = round((\n",
    "        sum(y_pred == test_labels) + \n",
    "        sum(y_pred == test_labels-1) + sum(y_pred == test_labels+1)\n",
    "        )/test_labels.shape[0],3)*100\n",
    "    print(\"The kNN classifier with k=\" + str(k) + \" had an accuracy of \" \n",
    "          + str(acc) + \"% (and a +/-1 grade accuracy of \" + str(acc2) + \"%).\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best model: n = 1\n",
    "kNN = KNeighborsClassifier(n_neighbors = 1)\n",
    "kNN.fit(train_features, train_labels)\n",
    "\n",
    "# print the values the model gets wrong\n",
    "print(\"\\nThe ratings misclassified by the classifier were: \")\n",
    "y_pred = kNN.predict(test_features)\n",
    "errors = test_labels[y_pred != test_labels]\n",
    "print(str(y_pred[y_pred != test_labels]) + \"-- predicted values\")\n",
    "print(str(errors) + \"-- actual values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred))*0.2, y_pred+0.1-np.random.uniform(size=len(y_pred))*0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# logistic regression model\n",
    "LogRegmodel = LogisticRegression(multi_class = 'multinomial', solver='newton-cg')\n",
    "LogRegmodel.fit(train_features, train_labels)\n",
    "\n",
    "print(\"The logistic regression model achieves an R2 value of \" + str(LogRegmodel.score(test_features, test_labels)))\n",
    "\n",
    "# print the values the model gets wrong\n",
    "print(\"\\nThe ratings misclassified by the classifier were: \")\n",
    "y_pred = LogRegmodel.predict(test_features)\n",
    "errors = test_labels[y_pred != test_labels]\n",
    "print(str(y_pred[y_pred != test_labels]) + \"-- predicted values\")\n",
    "print(str(errors) + \"-- actual values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred))*0.2, y_pred+0.1-np.random.uniform(size=len(y_pred))*0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(train_features, train_labels)\n",
    "\n",
    "print(\"The perceptron model achieves an R2 value of \" + str(perceptron.score(test_features, test_labels)))\n",
    "y_pred = perceptron.predict(test_features)\n",
    "\n",
    "# print the values the model gets wrong\n",
    "print(\"\\nThe ratings misclassified by the classifier were: \")\n",
    "y_pred = perceptron.predict(test_features)\n",
    "errors = test_labels[y_pred != test_labels]\n",
    "print(str(y_pred[y_pred != test_labels]) + \"-- predicted values\")\n",
    "print(str(errors) + \"-- actual values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='poly', degree=8)\n",
    "svc.fit(train_features, train_labels)\n",
    "\n",
    "print(\"The svm model achieves an R2 value of \" + str(svc.score(test_features, test_labels)))\n",
    "y_pred = svc.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred))*0.2, y_pred+0.1-np.random.uniform(size=len(y_pred))*0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(train_features, train_labels)\n",
    "\n",
    "print(\"The decision tree achieves an R2 value of \" + str(tree.score(test_features, test_labels)))\n",
    "y_pred = tree.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(test_labels+0.1-np.random.uniform(size=len(y_pred))*0.2, y_pred+0.1-np.random.uniform(size=len(y_pred))*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
